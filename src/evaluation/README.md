# Evaluation pipeline (SSL RCC project)

This directory contains the evaluation pipeline for SSL models (MoCo v3, DINOv3, iBOT, i-JEPA, etc.) and scripts for patient-level aggregation.

Main structure:

- `eval.py`  
  Test-only evaluation script for a single model (encoder + linear classifier).
  Reads a YAML config generated by `tools/auto_eval.py`, runs eval on patches and saves:
  - `metrics_<model>.json`
  - `report_per_class.json`
  - `cm_<model>.png`
  - `logits_test.npy`
  - `predictions.csv` (enriched with `wds_key` + metadata for XAI alignment)

- `tools/auto_eval.py`  
  Auto-discovery of runs in MLflow (`mlruns`), generation of eval YAMLs (`auto_configs/`) and optionally submit SLURM jobs calling `eval.py` via `eval_models.sbatch`.

- `tools/batch_patient_aggregation.py`  
  Patch → patient aggregation, using all patches and **always excluding** `NOT_TUMOR` from final decision.  
  Produces patient-level metrics, patient-level confusion matrix and summary CSV.

- `eval_models.sbatch`  
  SLURM script that:
  - creates/updates dedicated venv (`.venvs/eval`),
  - installs `requirements_eval.txt`,
  - runs `eval.py --config "$CFG_PATH"` where `CFG_PATH` is exported by `tools/auto_eval.py`.

- `ssl_linear_loader.py`  
  Wrapper to load SSL backbone (ResNet or ViT) + linear head from MoCo/DINO/iBOT/iJEPA checkpoints, with auto-swap ResNet↔ViT logic when needed.

- `auto_configs/`  
  Evaluation YAMLs automatically generated by `tools/auto_eval.py`.  
  Each file corresponds to a single MLflow run (a specific ablation / model).


## Environment and requirements

The SLURM scripts (`eval_models.sbatch`) automatically handle:

- venv creation: `.venvs/eval`
- dependency installation: `pip install -r src/evaluation/requirements_eval.txt`

If you want to run locally (without SLURM), you can:

```bash
cd /home/mla_group_01/rcc-ssrl
python3 -m venv .venvs/eval
source .venvs/eval/bin/activate
pip install --upgrade pip
pip install -r src/evaluation/requirements_eval.txt


Execution: automatic evaluation
1. Generation of eval config + submit to SLURM

Example: run automatic evaluation on all runs within an MLflow experiment (exp_20251118_105221_dino_v3):

python /home/mla_group_01/src/evaluation/tools/auto_eval.py \
  --mlruns-root "/beegfs-scratch/mla_group_01/workspace/mla_group_01/wsi-ssrl-rcc_project/outputs/mlruns/experiments/exp_20251213_121650_i_jepa" \
  --submit


Execution: patient-level aggregation

Once eval.py has produced predictions.csv and (optionally) logits_test.npy for each run, you can aggregate at patient level.

1. Aggregation over an entire experiment (all ablations/runs)

Example:

python /home/mla_group_01/src/evaluation/tools/batch_patient_aggregation.py \
  --mlruns-root /home/mla_group_01/outputs/mlruns/experiments/exp_20260122-131927/exp_transfer \
  --method prob_sum


Generate Reliability Diagram + ECE and Risk–Coverage (patch and/or patient)

source /home/mla_group_01/.venvs/eval/bin/activate  # recommended if it exists
python /home/mla_group_01/src/evaluation/tools/calibration_and_coverage.py \
  --run-dir /beegfs-scratch/mla_group_01/workspace/mla_group_01/wsi-ssrl-rcc_project/outputs/mlruns/experiments/ablation_final/exp_20251209_234736_dino_v3/exp_dino_v3_abl03/eval/dino_v3_ssl_linear_best/20251211_104905 \
  --out-dir /home/mla_group_01/src/evaluation/results/dino_v3_best_calibration \
  --n-bins 15 \
  --title "Dino v3 (patch-level)"
